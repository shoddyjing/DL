{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.31682708 0.69627909]\n"
     ]
    }
   ],
   "source": [
    "#ch03.04 page 56-63\n",
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "'''\n",
    "multi-dimension array:\n",
    "    1.get the dimension of an array: np.dim(x)\n",
    "    2.get the size of every dimension（turple,(2,)）: x.shape\n",
    "\n",
    "matrix（2-dimensional array） multiplication:\n",
    "    1.点乘（即矩阵对应元素相乘），结果为数量积，a、b形状相同np.multiply(a,b)\n",
    "    2.叉乘（又称矩阵乘法、点积），结果为矢量积，np.dot(x,w)\n",
    "    3.“*”在数组*数组类型时表示点乘，矩阵*矩阵类型时表示叉乘\n",
    "    \n",
    "'''\n",
    "\n",
    "#step function\n",
    "def step_function(x):# if x>=0:  return 1 elif x<0: return 0\n",
    "    #y=np.array(x>0)\n",
    "    y=np.array(x>0, dtype=np.int)         \n",
    "    return y\n",
    "\n",
    "def sigmoid(x):\n",
    "    y=1/(1+np.exp(-x))  #和非数组计算的广播功能\n",
    "    return y\n",
    "\n",
    "#Rectified Linear Unit function\n",
    "def ReLU(x):\n",
    "  #  y=np.array(x)\n",
    "  #  y[y<=0]=0.0\n",
    "  #  return y\n",
    "    return(np.maximum(x,0)) \n",
    "\n",
    "#1. identity_function 恒等函数，apply to regression problems 适合回归问题\n",
    "def identity(a):\n",
    "    return a\n",
    "\n",
    "#2. softmax_function ，apply to classification problems 适合分类问题\n",
    "def softmax(a):\n",
    "    c=np.max(a)\n",
    "    exp_a=np.exp(a-c)    #通过减去最大值防止因e指数后各个数值相差太大而出现的溢出现象。\n",
    "    sum_exp_a=np.sum(exp_a)\n",
    "    y=exp_a/sum_exp_a  #sum_exp_a为一个常数，此处使用了广播功能\n",
    "    return y           #sum(y)=1,且y的大小顺序同输入a(y=e**a为单调递增函数)，所以softmax层有时会被省略\n",
    "    \n",
    "def init_network(): #输入层两个神经元，第一层3个神经元，第二层两个神经元，输出层两个神经元\n",
    "    network={}    #创建字典型变量存储权重和偏置值\n",
    "    network['w1']=np.array([[0.1,0.3,0.5],[0.2,0.4,0.6]])\n",
    "    network['b1']=np.array([0.1,0.2,0.3])\n",
    "    network['w2']=np.array([[0.1,0.4],[0.2,0.5],[0.3,0.6]])\n",
    "    network['b2']=np.array([0.1,0.2])\n",
    "    network['w3']=np.array([[0.1,0.3],[0.2,0.4]])\n",
    "    network['b3']=np.array([0.1,0.2])\n",
    "    return network\n",
    "\n",
    "def forward(network,x):\n",
    "    w1,w2,w3=network['w1'],network['w2'],network['w3']\n",
    "    b1,b2,b3=network['b1'],network['b2'],network['b3']\n",
    "    \n",
    "    a1=np.dot(x,w1)+b1\n",
    "    z1=sigmoid(a1)     #中间层值为a->z，输出层为y\n",
    "    a2=np.dot(z1,w2)+b2\n",
    "    z2=sigmoid(a2)\n",
    "    a3=np.dot(z2,w3)+b3\n",
    "    y=identity(a3)\n",
    "    return y\n",
    "\n",
    "network=init_network()\n",
    "x=np.array([1.0,0.5])\n",
    "y=forward(network,x)\n",
    "print(y)\n",
    "\n",
    "#try:    \n",
    "#    !jupyter nbconvert --to python 3layered_neural_network_forward.ipynb\n",
    "#    # python即转化为.py，script即转化为.html\n",
    "#except:\n",
    "#    pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
